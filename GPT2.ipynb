{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggolani/ML/blob/main/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbU4kcLemKRY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e6WNeNnLvGj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKNdxtm3mjKT"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur3f4QfjScmx"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter('runs/gpt2')\n",
        "\n",
        "def log_weights_histograms(model, global_step):\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Tag format is 'weights/layer_name'\n",
        "            writer.add_histogram(f'weights/{name}', param.data, global_step)\n",
        "\n",
        "def log_gradients_histograms(model, global_step):\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad and param.grad is not None:\n",
        "            # Tag format is 'gradients/layer_name'\n",
        "            writer.add_histogram(f'gradients/{name}', param.grad, global_step)\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPncnxp1nDWT"
      },
      "outputs": [],
      "source": [
        "#hyperparameters for GPT2-124M\n",
        "n_vocab = tokenizer.vocab_size\n",
        "embed_dim = 768\n",
        "seq_len = 256\n",
        "n_heads = 12\n",
        "n_blocks = 12\n",
        "batch_size = 32\n",
        "dropout = 0 # range [0-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ5mvbk0nZA5"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddjBTxGnnr0E"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "text = requests.get('https://www.gutenberg.org/cache/epub/829/pg829.txt').text\n",
        "gtTokens = torch.tensor(tokenizer.encode(text))\n",
        "print(len(gtTokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6auDciVEn4MY"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.9\n",
        "train_data = torch.tensor([], dtype=torch.long)\n",
        "test_data = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  shard_max = math.floor(len(gtTokens) / 10 * (i+1))\n",
        "  shard_min = math.floor(len(gtTokens) / 10 * i)\n",
        "  train_max = math.ceil(shard_min + (shard_max - shard_min) * train_ratio)\n",
        "  train_data = torch.cat((train_data, gtTokens[shard_min:train_max]))\n",
        "  test_data = torch.cat((test_data, gtTokens[train_max+1:shard_max-1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saBD2yIXh9C4"
      },
      "outputs": [],
      "source": [
        "# a function that returns a batch of data samples\n",
        "def get_data_batch(training=True):\n",
        "\n",
        "  # pick the dataset to use\n",
        "  if training:\n",
        "    data = train_data\n",
        "  else:\n",
        "    data = test_data\n",
        "\n",
        "  # pick random indices to start\n",
        "  ix = torch.randint(len(data)-seq_len,size=(batch_size,))\n",
        "\n",
        "  # get the data and targets (via broadcasting outer product)\n",
        "  X = data[ix[:,None] + torch.arange(seq_len)]\n",
        "  y = data[ix[:,None] + torch.arange(1,seq_len+1)]\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkR39MsSplJp"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        self.QKV = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n",
        "        self.W0 = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, E = x.shape # [batch, seq_len, embed_dim]\n",
        "        qkv = self.QKV(x)\n",
        "        q,k,v = torch.split(qkv, E, dim=2)\n",
        "\n",
        "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "        dropp=dropout if self.training==True else 0\n",
        "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=dropp) # [B, nHeads, T, head_dim]\n",
        "\n",
        "        # recombine heads: (B, nHeads, T, head_dim) -> [B, T, E]\n",
        "        out = out.transpose(1,2).view(B, T, E)\n",
        "\n",
        "        # finally, linearly mix the attention heads\n",
        "        out = self.W0(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUqd60-DwIaW"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layernorm_1 = nn.LayerNorm(embed_dim, eps=1e-5)\n",
        "        self.attn = MultiHeadAttention()\n",
        "\n",
        "        self.layernorm_2 = nn.LayerNorm(embed_dim, eps=1e-5)\n",
        "\n",
        "        self.mlp_1 = nn.Linear(embed_dim, 4*embed_dim, bias=True)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.mlp_2 = nn.Linear(4*embed_dim, embed_dim, bias=True)\n",
        "\n",
        "        #n transformer block dropout\n",
        "        self.trn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_att = self.layernorm_1(x)\n",
        "        x_att = self.trn_dropout(self.attn(x_att)) + x\n",
        "\n",
        "        x_ff = self.layernorm_2(x_att)\n",
        "        x_ff = self.mlp_2(self.gelu( self.mlp_1(x_ff) )) # expansion-contraction\n",
        "        x_ff = x_att + self.trn_dropout(x_ff) #n dropout the MLP and add back to the embeddings vectors\n",
        "\n",
        "        return x_ff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAnBdBh-2aO2"
      },
      "outputs": [],
      "source": [
        "class LLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.wte = nn.Embedding(n_vocab, embed_dim)\n",
        "        self.wpe = nn.Embedding(seq_len, embed_dim)\n",
        "        #n dropout\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.transformerBlocks = nn.Sequential(*[TransformerBlock() for _ in range(n_blocks)])\n",
        "\n",
        "        self.layernorm_final = nn.LayerNorm(embed_dim, eps=1e-5)\n",
        "\n",
        "        self.final_head = nn.Linear(embed_dim, n_vocab, bias=False)\n",
        "        self.final_head.weight = nn.Parameter(self.wte.weight)\n",
        "\n",
        "        self.apply(self.weightInits)\n",
        "\n",
        "    def weightInits(self, module):\n",
        "        # revisit initialization to optimize for choice of activation function\n",
        "        if isinstance(module, nn.Linear):\n",
        "          nn.init.xavier_normal_(module.weight)\n",
        "          if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "        if isinstance(module, nn.Embedding):\n",
        "          nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "\n",
        "    def forward(self, idx):\n",
        "        token_embeddings = self.wte(idx)\n",
        "        pos_embeddings = self.wpe(torch.arange(idx.shape[-1], device=device))\n",
        "        x = token_embeddings + pos_embeddings\n",
        "        x = self.emb_dropout(x) #n dropout after summing E+P\n",
        "\n",
        "        x = self.transformerBlocks(x)\n",
        "        x = self.layernorm_final(x)\n",
        "\n",
        "        logits = self.final_head(x)\n",
        "\n",
        "        outputs = F.log_softmax(logits/np.sqrt(embed_dim),dim=-1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -seq_len:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.exp(logits)\n",
        "\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbTetA1r6V8W"
      },
      "outputs": [],
      "source": [
        "\n",
        "CHECKPOINT_PATH = '/content/gdrive/My Drive/my_checkpoint.pth'\n",
        "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd1IqSDg3t4K"
      },
      "outputs": [],
      "source": [
        "# create an instance and test with some data\n",
        "model = LLM().to(device)\n",
        "#load from gdrive if reusing checkpointed model\n",
        "#checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device(device))\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jurX1xzHT0Sg"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqTElK80TMGZ"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.NLLLoss().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=.001, weight_decay=.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WUYuqpThHZL5"
      },
      "outputs": [],
      "source": [
        "num_epochs = 4001\n",
        "\n",
        "# initialize losses\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # get a batch of data\n",
        "  X,y = get_data_batch()\n",
        "\n",
        "  # move data to GPU\n",
        "  X,y = X.to(device), y.to(device)\n",
        "\n",
        "  # clear previous gradients\n",
        "  model.zero_grad(set_to_none=True)\n",
        "\n",
        "  # forward pass\n",
        "  log_probs = model(X)\n",
        "\n",
        "  # calculate the losses on the (reshaped) targets\n",
        "  loss = loss_function(log_probs.view(-1,log_probs.shape[-1]),y.view(-1))\n",
        "\n",
        "  # backprop\n",
        "  loss.backward()\n",
        "\n",
        "  if epoch%100==0:\n",
        "    log_weights_histograms(model, epoch)\n",
        "    log_gradients_histograms(model, epoch)\n",
        "    writer.add_scalar('loss', loss, epoch)\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  # store the per-sample loss\n",
        "  train_loss.append( loss.item() )\n",
        "\n",
        "  if epoch%1000==0:\n",
        "    save_checkpoint(model, optimizer, epoch, loss, CHECKPOINT_PATH)\n",
        "\n",
        "  # evaluate the model with the test set\n",
        "  if epoch%100==0:\n",
        "\n",
        "    with torch.no_grad():\n",
        "      X,y = get_data_batch(False)       # False -> testset data\n",
        "      X,y = X.to(device), y.to(device)  # push it to the GPU\n",
        "      out = model(X)                    # forward pass\n",
        "      thisloss = loss_function(out.view(-1,out.shape[-1]),y.view(-1)) # calculate loss\n",
        "      test_loss.append( thisloss.item() )\n",
        "\n",
        "      # update our progress :)\n",
        "      print(f'Epoch {epoch:4}, train loss: {train_loss[-1]:5.2f}, test loss: {test_loss[-1]:5.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XdjwacoHZOi"
      },
      "outputs": [],
      "source": [
        "# plot the losses\n",
        "plt.plot(train_loss,'k',label='Train loss')\n",
        "plt.plot(range(0,num_epochs,50),test_loss,'rs-',markerfacecolor='w',markersize=8,label='Test loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Epoch',ylabel='Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ql-aaqpZBAN"
      },
      "outputs": [],
      "source": [
        "prompt = 'I find likewise that your printer has been so'\n",
        "in2gpt = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
        "\n",
        "output = model.generate(in2gpt,max_new_tokens=5)\n",
        "print(tokenizer.decode(output[0]).replace('\\r','\\n'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}