{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNT2Bok3vbdzPHhkRX94jH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggolani/ML/blob/main/GPT2_FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoModelForCausalLM,GPT2Tokenizer\n",
        "import requests\n",
        "\n",
        "# vector plots\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ],
      "metadata": {
        "id": "aoocnKDi-2RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained GPT-2 model and tokenizer\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "bHKfbxSx-B5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "seq_len    = 256 # max sequence length\n",
        "batch_size =  16\n",
        "\n",
        "# use GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "7AInxn0_kzOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text\n",
        "# Gulliver's travels :)\n",
        "text = requests.get('https://www.gutenberg.org/cache/epub/829/pg829.txt').text\n",
        "\n",
        "\n",
        "gtTokens = tokenizer.encode(text,return_tensors='pt')\n",
        "print(gtTokens.shape)\n",
        "\n",
        "# but the rest of the code is setup for dimensionless tensors\n",
        "gtTokens = gtTokens[0]\n",
        "print(gtTokens.shape)"
      ],
      "metadata": {
        "id": "lP_txTZrkzLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the most frequent 100 tokens\n",
        "uniq,counts = np.unique(gtTokens,return_counts=True)\n",
        "freqidx = np.argsort(counts)[::-1]\n",
        "top100 = uniq[freqidx[:100]]"
      ],
      "metadata": {
        "id": "RxWzr6U4ky9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numreps =  10 # number of random repetitions\n",
        "numtoks = 100 # output length\n",
        "\n",
        "# random starting tokens\n",
        "randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n",
        "\n",
        "# generate some data\n",
        "out = gpt2.generate(\n",
        "  randstarts,\n",
        "  max_length = numtoks+1,\n",
        "  min_length = numtoks+1,\n",
        "  do_sample  = True,\n",
        "  bad_words_ids = [tokenizer.encode(tokenizer.eos_token)],\n",
        "  pad_token_id = tokenizer.encode(tokenizer.eos_token)[0]\n",
        ").cpu()\n",
        "\n",
        "# calculate and report the percentage\n",
        "percentFreqTokens_pre = np.mean(100*np.isin(out[:,1:],top100).flatten())\n",
        "print(f\"Gulliver's travels common tokens appeared in {percentFreqTokens_pre}% of new tokens.\")"
      ],
      "metadata": {
        "id": "bYUEiUJIf2vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KLDivergenceLoss_x(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # mask: 1 if token contains a target, 0 otherwise\n",
        "    self.mask = torch.zeros(tokenizer.vocab_size, device=device)\n",
        "    for t in range(tokenizer.vocab_size):\n",
        "      thistoken = tokenizer.decode([t])\n",
        "      if 'x' in thistoken:\n",
        "        self.mask[t] = 1\n",
        "\n",
        "    # normalize to pdist\n",
        "    self.mask = self.mask/torch.sum(self.mask)\n",
        "\n",
        "  def forward(self, log_probs):\n",
        "    # assumes log-softmax-prob input!\n",
        "    return F.kl_div(log_probs, self.mask, reduction='batchmean')"
      ],
      "metadata": {
        "id": "SFVusrX5qM1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a loss function instance\n",
        "#loss_function = KLDivergenceLoss_x().to(device)"
      ],
      "metadata": {
        "id": "VwDKUSRWF_LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move the model to the GPU\n",
        "gpt2 = gpt2.to(device)"
      ],
      "metadata": {
        "id": "QlWxZTAaky6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check out some text\n",
        "prompt = 'I cannot believe that'\n",
        "in2gpt = tokenizer.encode(prompt,return_tensors='pt').to(device)\n",
        "\n",
        "output = gpt2.generate(in2gpt,max_length=100,pad_token_id=50256,do_sample=True).cpu()\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "y1c4MlmemixA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # create the optimizer functions (note the small learning rate)\n",
        "optimizer = torch.optim.AdamW(gpt2.parameters(), lr=5e-5, weight_decay=.01)\n",
        "#HF models provide their own inbuilt loss function"
      ],
      "metadata": {
        "id": "qpcC47LUy7w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1234\n",
        "\n",
        "# initialize losses\n",
        "train_loss = np.zeros(num_samples)\n",
        "\n",
        "for sampli in range(num_samples):\n",
        "\n",
        "  # get a batch of data\n",
        "  ix = torch.randint(len(gtTokens)-seq_len,size=(batch_size,))\n",
        "  X  = gtTokens[ix[:,None] + torch.arange(seq_len)]\n",
        "\n",
        "  # move data to GPU\n",
        "  X = X.to(device)\n",
        "\n",
        "  # clear previous gradients\n",
        "  gpt2.zero_grad()\n",
        "\n",
        "  # forward pass (Hugging Face shifts X internally to get y)\n",
        "\n",
        "  output = gpt2(X, labels=X)\n",
        "  # calculate the losses\n",
        "  loss = output.loss\n",
        "  # if using KL-divergence custom function\n",
        "  #logits = gpt2(X, labels=X).logits\n",
        "  #logits_reshape = logits.view(-1,tokenizer.vocab_size)\n",
        "  #logprobs_reshape = F.log_softmax(logits_reshape,dim=-1)\n",
        "  #loss = loss_function(logprobs_reshape)\n",
        "\n",
        "  # backprop\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # store the per-sample loss\n",
        "  train_loss[sampli] = loss.item()\n",
        "\n",
        "  # update progress display\n",
        "  if sampli%77==0:\n",
        "    print(f'Sample {sampli:4}/{num_samples}, train loss: {train_loss[sampli]:.4f}')"
      ],
      "metadata": {
        "id": "WUYuqpThHZL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oMKA4BKnky1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the losses\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(train_loss,'k',markersize=8)\n",
        "\n",
        "plt.gca().set(xlabel='Data sample',ylabel='Train loss',xlim=[-1,num_samples])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5XdjwacoHZOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1wZg_eplZBDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Qualtative assessment\n",
        "prompt = 'I cannot believe that'\n",
        "in2gpt = tokenizer.encode(prompt,return_tensors='pt').to(device)\n",
        "\n",
        "output = gpt2.generate(in2gpt,max_length=100,pad_token_id=50256)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "4ql-aaqpZBAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eA3Ak8LZA82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate percentage of GT tokens generated"
      ],
      "metadata": {
        "id": "IY3ph6roofPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random starting tokens\n",
        "randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n",
        "\n",
        "# generate some data\n",
        "out = gpt2.generate(\n",
        "  randstarts,\n",
        "  max_length = numtoks+1,\n",
        "  min_length = numtoks+1,\n",
        "  do_sample  = True,\n",
        "  bad_words_ids = [tokenizer.encode(tokenizer.eos_token)],\n",
        "  pad_token_id = tokenizer.encode(tokenizer.eos_token)[0]\n",
        ").cpu()\n",
        "\n",
        "\n",
        "for o in out:\n",
        "  print('\\n*** Next batch of output:')\n",
        "  print(tokenizer.decode(o))"
      ],
      "metadata": {
        "id": "fVcT-MNE4OHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate and report the percentage\n",
        "percentFreqTokens_pst = np.mean(100*np.isin(out[:,1:],top100).flatten())\n",
        "\n",
        "print(f'Common GT tokens usage went from {percentFreqTokens_pre:.2f}% to {percentFreqTokens_pst:.2f}% after fine-tuning.')"
      ],
      "metadata": {
        "id": "6tMVZm72WjEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vkbGVHeVofFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}